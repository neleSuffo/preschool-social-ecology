{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ 1: How much time do children spend alone?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis parameters:\n",
      "- Window size: 30 seconds = 90 sampled frames\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/nele_pauline_suffo/projects/naturalistic-social-analysis/src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import re\n",
    "from rq_01_queries import extract_segments_with_buffering\n",
    "\n",
    "TOTAL_FRAMES = 875_887\n",
    "FPS = 30\n",
    "SAMPLE_RATE = 10  # every 10th frame\n",
    "WINDOW_SIZE = 30  # window size in seconds\n",
    "\n",
    "# Calculated values\n",
    "WINDOW_SIZE_FRAMES = WINDOW_SIZE * FPS  # e.g., 30s * 30fps = 900 frames\n",
    "WINDOW_SIZE_SAMPLED = WINDOW_SIZE_FRAMES // SAMPLE_RATE  # e.g., 900 // 10 = 90 frames\n",
    "\n",
    "print(f\"Analysis parameters:\")\n",
    "print(f\"- Window size: {WINDOW_SIZE} seconds = {WINDOW_SIZE_SAMPLED} sampled frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"/home/nele_pauline_suffo/projects/naturalistic-social-analysis/src/results/rq_01/frame_level_social_interactions.csv\")\n",
    "age_df = pd.read_csv(\"/home/nele_pauline_suffo/ProcessedData/age_group.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames with ONLY child faces: 52,395 (5.98%)\n",
      "Frames with ONLY adult faces: 63,510 (7.25%)\n",
      "Frames with BOTH face types: 17,967 (2.05%)\n",
      "Frames with NO faces: 742,015 (84.72%)\n",
      "Analysis check : 0 frames deviation (should be 0)\n"
     ]
    }
   ],
   "source": [
    "# Calculate mutually exclusive counts\n",
    "only_child_face = (results_df['face_frame_category'] == 'only_child').sum()\n",
    "only_adult_face = (results_df['face_frame_category'] == 'only_adult').sum()\n",
    "both_faces = (results_df['face_frame_category'] == 'both_faces').sum()\n",
    "no_faces = (results_df['face_frame_category'] == 'no_faces').sum()\n",
    "analysis_check_face = only_child_face + only_adult_face + both_faces + no_faces - TOTAL_FRAMES\n",
    "\n",
    "print(f\"Frames with ONLY child faces: {only_child_face:,} ({only_child_face / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Frames with ONLY adult faces: {only_adult_face:,} ({only_adult_face / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Frames with BOTH face types: {both_faces:,} ({both_faces / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Frames with NO faces: {no_faces:,} ({no_faces / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Analysis check : {analysis_check_face} frames deviation (should be 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames with ONLY child persons: 2,279 (0.26%)\n",
      "Frames with ONLY adult persons: 9,590 (1.09%)\n",
      "Frames with BOTH person types: 1,031 (0.12%)\n",
      "Frames with NO persons: 862,987 (98.53%)\n",
      "Analysis check : 0 frames deviation (should be 0)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the numbers\n",
    "only_child_person = (results_df['person_frame_category'] == 'only_child').sum()\n",
    "only_adult_person = (results_df['person_frame_category'] == 'only_adult').sum()\n",
    "both_persons = (results_df['person_frame_category'] == 'both_persons').sum()\n",
    "no_persons = (results_df['person_frame_category'] == 'no_persons').sum()\n",
    "analysis_check_person = only_child_person + only_adult_person + both_persons + no_persons - TOTAL_FRAMES\n",
    "\n",
    "print(f\"Frames with ONLY child persons: {only_child_person:,} ({only_child_person / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Frames with ONLY adult persons: {only_adult_person:,} ({only_adult_person / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Frames with BOTH person types: {both_persons:,} ({both_persons / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Frames with NO persons: {no_persons:,} ({no_persons / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Analysis check : {analysis_check_person} frames deviation (should be 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Face and Person Presence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames with ONLY child present: 53,836 (6.15%)\n",
      "Frames with ONLY adult present: 70,818 (8.09%)\n",
      "Frames with BOTH present: 19,438 (2.22%)\n",
      "Frames with NO ONE present: 731,795 (83.55%)\n",
      "Analysis check: 0 frames deviation (should be 0)\n",
      "\n",
      "Summary Insights:\n",
      "Frames with ANY human presence: 144,092 (16.45%)\n",
      "Frames with child presence: 73,274 (8.37%)\n",
      "Frames with adult presence: 90,256 (10.30%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate combined presence patterns using the correct logic\n",
    "only_child_present = ((results_df['child_present'] == 1) & (results_df['adult_present'] == 0)).sum()\n",
    "only_adult_present = ((results_df['child_present'] == 0) & (results_df['adult_present'] == 1)).sum()\n",
    "both_present = ((results_df['child_present'] == 1) & (results_df['adult_present'] == 1)).sum()\n",
    "no_one_present = ((results_df['child_present'] == 0) & (results_df['adult_present'] == 0)).sum()\n",
    "analysis_check_combined = only_child_present + only_adult_present + both_present + no_one_present - TOTAL_FRAMES\n",
    "\n",
    "print(f\"Frames with ONLY child present: {only_child_present:,} ({only_child_present / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Frames with ONLY adult present: {only_adult_present:,} ({only_adult_present / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Frames with BOTH present: {both_present:,} ({both_present / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Frames with NO ONE present: {no_one_present:,} ({no_one_present / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Analysis check: {analysis_check_combined} frames deviation (should be 0)\")\n",
    "\n",
    "# Additional insights\n",
    "any_presence = only_child_present + only_adult_present + both_present\n",
    "print(f\"\\nSummary Insights:\")\n",
    "print(f\"Frames with ANY human presence: {any_presence:,} ({any_presence / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Frames with child presence: {only_child_present + both_present:,} ({(only_child_present + both_present) / TOTAL_FRAMES * 100:.2f}%)\")\n",
    "print(f\"Frames with adult presence: {only_adult_present + both_present:,} ({(only_adult_present + both_present) / TOTAL_FRAMES * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating segments...\n",
      "Created 5014 segments after buffering.\n"
     ]
    }
   ],
   "source": [
    "segments_df = extract_segments_with_buffering(results_df)\n",
    "segments_df.to_csv(\"/home/nele_pauline_suffo/projects/naturalistic-social-analysis/src/results/rq_01/segments_with_buffering.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_name</th>\n",
       "      <th>category</th>\n",
       "      <th>segment_start</th>\n",
       "      <th>segment_end</th>\n",
       "      <th>start_time_sec</th>\n",
       "      <th>end_time_sec</th>\n",
       "      <th>duration_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>25</td>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01</td>\n",
       "      <td>Co-present Silent</td>\n",
       "      <td>0</td>\n",
       "      <td>660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>25</td>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01</td>\n",
       "      <td>Alone</td>\n",
       "      <td>670</td>\n",
       "      <td>1240</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>41.333333</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>25</td>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01</td>\n",
       "      <td>Interacting</td>\n",
       "      <td>1250</td>\n",
       "      <td>1890</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>21.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>25</td>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01</td>\n",
       "      <td>Alone</td>\n",
       "      <td>1900</td>\n",
       "      <td>2280</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>12.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>25</td>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01</td>\n",
       "      <td>Interacting</td>\n",
       "      <td>2290</td>\n",
       "      <td>3730</td>\n",
       "      <td>76.333333</td>\n",
       "      <td>124.333333</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>25</td>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01</td>\n",
       "      <td>Alone</td>\n",
       "      <td>3740</td>\n",
       "      <td>9760</td>\n",
       "      <td>124.666667</td>\n",
       "      <td>325.333333</td>\n",
       "      <td>200.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>25</td>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01</td>\n",
       "      <td>Interacting</td>\n",
       "      <td>9770</td>\n",
       "      <td>9940</td>\n",
       "      <td>325.666667</td>\n",
       "      <td>331.333333</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>25</td>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01</td>\n",
       "      <td>Alone</td>\n",
       "      <td>9950</td>\n",
       "      <td>11870</td>\n",
       "      <td>331.666667</td>\n",
       "      <td>395.666667</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>25</td>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01</td>\n",
       "      <td>Interacting</td>\n",
       "      <td>11880</td>\n",
       "      <td>12480</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>25</td>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01</td>\n",
       "      <td>Alone</td>\n",
       "      <td>12490</td>\n",
       "      <td>13340</td>\n",
       "      <td>416.333333</td>\n",
       "      <td>444.666667</td>\n",
       "      <td>28.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>25</td>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01</td>\n",
       "      <td>Interacting</td>\n",
       "      <td>13350</td>\n",
       "      <td>14730</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>491.000000</td>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     video_id                              video_name           category  \\\n",
       "558        25  quantex_at_home_id255944_2022_03_08_01  Co-present Silent   \n",
       "559        25  quantex_at_home_id255944_2022_03_08_01              Alone   \n",
       "560        25  quantex_at_home_id255944_2022_03_08_01        Interacting   \n",
       "561        25  quantex_at_home_id255944_2022_03_08_01              Alone   \n",
       "562        25  quantex_at_home_id255944_2022_03_08_01        Interacting   \n",
       "563        25  quantex_at_home_id255944_2022_03_08_01              Alone   \n",
       "564        25  quantex_at_home_id255944_2022_03_08_01        Interacting   \n",
       "565        25  quantex_at_home_id255944_2022_03_08_01              Alone   \n",
       "566        25  quantex_at_home_id255944_2022_03_08_01        Interacting   \n",
       "567        25  quantex_at_home_id255944_2022_03_08_01              Alone   \n",
       "568        25  quantex_at_home_id255944_2022_03_08_01        Interacting   \n",
       "\n",
       "     segment_start  segment_end  start_time_sec  end_time_sec  duration_sec  \n",
       "558              0          660        0.000000     22.000000     22.000000  \n",
       "559            670         1240       22.333333     41.333333     19.000000  \n",
       "560           1250         1890       41.666667     63.000000     21.333333  \n",
       "561           1900         2280       63.333333     76.000000     12.666667  \n",
       "562           2290         3730       76.333333    124.333333     48.000000  \n",
       "563           3740         9760      124.666667    325.333333    200.666667  \n",
       "564           9770         9940      325.666667    331.333333      5.666667  \n",
       "565           9950        11870      331.666667    395.666667     64.000000  \n",
       "566          11880        12480      396.000000    416.000000     20.000000  \n",
       "567          12490        13340      416.333333    444.666667     28.333333  \n",
       "568          13350        14730      445.000000    491.000000     46.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_df[segments_df['video_id'] == 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " #--- Step 1: Extract child_id from video_name if not already present ---\n",
    "def extract_child_id(video_name):\n",
    "    match = re.search(r'id(\\d{6})', video_name)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "segments_df['child_id'] = segments_df['video_name'].apply(extract_child_id)\n",
    "\n",
    "# Keep only first occurrence of each child_id in age_df\n",
    "age_df = age_df.drop_duplicates(subset='child_id', keep='first')[['child_id', 'age_at_recording']]\n",
    "\n",
    "# Ensure both are strings for merging\n",
    "segments_df['child_id'] = segments_df['child_id'].astype(str)\n",
    "age_df['child_id'] = age_df['child_id'].astype(str)\n",
    "\n",
    "# --- Step 2: Aggregate duration per child and category (for filtering) ---\n",
    "duration_by_child_category = (\n",
    "    segments_df.groupby(['child_id', 'category'])['duration_sec']\n",
    "    .sum()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Only keep age data for children that have segments\n",
    "children_with_segments = set(duration_by_child_category.index)\n",
    "age_df_filtered = age_df[age_df['child_id'].isin(children_with_segments)]\n",
    "\n",
    "# Sort by age and save to CSV\n",
    "age_df_filtered = age_df_filtered.sort_values(by='age_at_recording')\n",
    "age_df_filtered.to_csv(\"/home/nele_pauline_suffo/projects/naturalistic-social-analysis/src/results/rq_01/child_id_age_mapping.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50143 KCHI utterances\n",
      "Columns available: ['vocalization_id', 'video_id', 'speaker', 'start_time_seconds', 'end_time_seconds', 'phonemes', 'syllables', 'words', 'model_id']\n",
      "Sample data:\n",
      "   vocalization_id  video_id speaker  start_time_seconds  end_time_seconds  \\\n",
      "0                1         1    KCHI              73.708            81.010   \n",
      "1                2         1    KCHI              83.865            83.992   \n",
      "2                3         1    KCHI              88.012            95.749   \n",
      "3                4         1    KCHI             102.781           103.493   \n",
      "4                5         1    KCHI             120.493           128.139   \n",
      "\n",
      "   phonemes  syllables  words  model_id  \n",
      "0     57.98      29.46  20.05         4  \n",
      "1      7.72       3.59   2.20         4  \n",
      "2     44.89      20.21  15.81         4  \n",
      "3      7.54       3.22   2.24         4  \n",
      "4     60.41      29.94  20.60         4  \n",
      "\n",
      "Assigning social categories to utterances...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50143 KCHI utterances\n",
      "Columns available: ['vocalization_id', 'video_id', 'speaker', 'start_time_seconds', 'end_time_seconds', 'phonemes', 'syllables', 'words', 'model_id']\n",
      "Sample data:\n",
      "   vocalization_id  video_id speaker  start_time_seconds  end_time_seconds  \\\n",
      "0                1         1    KCHI              73.708            81.010   \n",
      "1                2         1    KCHI              83.865            83.992   \n",
      "2                3         1    KCHI              88.012            95.749   \n",
      "3                4         1    KCHI             102.781           103.493   \n",
      "4                5         1    KCHI             120.493           128.139   \n",
      "\n",
      "   phonemes  syllables  words  model_id  \n",
      "0     57.98      29.46  20.05         4  \n",
      "1      7.72       3.59   2.20         4  \n",
      "2     44.89      20.21  15.81         4  \n",
      "3      7.54       3.22   2.24         4  \n",
      "4     60.41      29.94  20.60         4  \n",
      "\n",
      "Assigning social categories to utterances...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'start_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start_time'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Assign category and child_id to each utterance\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAssigning social categories to utterances...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m vocalizations_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchild_id\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mvocalizations_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43massign_category\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Check if 'words' column exists, otherwise use word count from utterance\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocalizations_df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/pandas/core/frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9412\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9414\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   9415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9416\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9421\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   9422\u001b[0m )\n\u001b[0;32m-> 9423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/pandas/core/apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/pandas/core/apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/pandas/core/apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    816\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m, in \u001b[0;36massign_category\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03mAssigns social category to each vocalization based on temporal overlap with segments.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    pd.Series: {'category': dominant_category, 'child_id': corresponding_child_id}\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m vid \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 40\u001b[0m utterance_start \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     41\u001b[0m utterance_end \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Find overlapping segments\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/pandas/core/series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/pandas/core/series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start_time'"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Load vocalizations table ---\n",
    "db_path = '/home/nele_pauline_suffo/outputs/quantex_inference/inference.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "vocalizations_df = pd.read_sql(\"SELECT * FROM Vocalizations WHERE speaker = 'KCHI'\", conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Loaded {len(vocalizations_df)} KCHI utterances\")\n",
    "print(f\"Columns available: {list(vocalizations_df.columns)}\")\n",
    "print(f\"Sample data:\")\n",
    "print(vocalizations_df.head())\n",
    "\n",
    "# --- Prepare segments_df for lookup ---\n",
    "segments_lookup = segments_df[[\"video_id\", \"start_time_sec\", \"end_time_sec\", \"category\", \"child_id\"]].copy()\n",
    "\n",
    "def assign_category(row):\n",
    "    \"\"\"\n",
    "    Assigns social category to each vocalization based on temporal overlap with segments.\n",
    "    \n",
    "    Logic:\n",
    "    1. Find all segments in the same video that temporally overlap with the utterance\n",
    "    2. Calculate the overlap duration for each overlapping segment\n",
    "    3. Assign the category of the segment with the maximum overlap\n",
    "    \n",
    "    Temporal overlap condition:\n",
    "    - Utterance: [start_time, end_time]\n",
    "    - Segment: [start_time_sec, end_time_sec]\n",
    "    - Overlap exists if: utterance_end > segment_start AND utterance_start < segment_end\n",
    "    \n",
    "    Args:\n",
    "        row: Vocalization row with video_id, start_time, end_time\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: {'category': dominant_category, 'child_id': corresponding_child_id}\n",
    "    \"\"\"\n",
    "    vid = row['video_id']\n",
    "    utterance_start = row['start_time']\n",
    "    utterance_end = row['end_time']\n",
    "    \n",
    "    # Find overlapping segments\n",
    "    overlapping_segments = segments_lookup[\n",
    "        (segments_lookup['video_id'] == vid) &\n",
    "        (segments_lookup['end_time_sec'] > utterance_start) &      # Segment ends after utterance starts\n",
    "        (segments_lookup['start_time_sec'] < utterance_end)        # Segment starts before utterance ends\n",
    "    ].copy()\n",
    "    \n",
    "    if overlapping_segments.empty:\n",
    "        return pd.Series({'category': 'unknown', 'child_id': np.nan})\n",
    "    \n",
    "    # Calculate overlap duration for each segment\n",
    "    overlapping_segments['overlap_duration'] = overlapping_segments.apply(\n",
    "        lambda seg: max(0, \n",
    "                       min(utterance_end, seg['end_time_sec']) -           # End of overlap\n",
    "                       max(utterance_start, seg['start_time_sec'])         # Start of overlap\n",
    "                       ), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Find segment with maximum overlap\n",
    "    max_overlap_idx = overlapping_segments['overlap_duration'].idxmax()\n",
    "    dominant_segment = overlapping_segments.loc[max_overlap_idx]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'category': dominant_segment['category'], \n",
    "        'child_id': dominant_segment['child_id']\n",
    "    })\n",
    "\n",
    "# Assign category and child_id to each utterance\n",
    "print(\"\\nAssigning social categories to utterances...\")\n",
    "vocalizations_df[['category', 'child_id']] = vocalizations_df.apply(assign_category, axis=1)\n",
    "\n",
    "# Check if 'words' column exists, otherwise use word count from utterance\n",
    "if 'words' in vocalizations_df.columns:\n",
    "    print(\"Using existing 'words' column\")\n",
    "    word_column = 'words'\n",
    "else:\n",
    "    print(\"'words' column not found, counting words from 'utterance' column\")\n",
    "    vocalizations_df['word_count'] = vocalizations_df['utterance'].str.split().str.len()\n",
    "    word_column = 'word_count'\n",
    "\n",
    "# --- Aggregate: total words per category ---\n",
    "words_by_category = vocalizations_df.groupby('category')[word_column].sum()\n",
    "words_by_child_cat = vocalizations_df.groupby(['child_id', 'category'])[word_column].sum().unstack(fill_value=0)\n",
    "\n",
    "# --- Percentages ---\n",
    "total_words = vocalizations_df[word_column].sum()\n",
    "words_by_category_pct = (words_by_category / total_words * 100).round(2)\n",
    "\n",
    "words_by_child = vocalizations_df.groupby('child_id')[word_column].sum()\n",
    "words_by_child_cat_pct = words_by_child_cat.div(words_by_child, axis=0).multiply(100).round(2)\n",
    "\n",
    "# --- Print summary ---\n",
    "print(f\"\\n=== ASSIGNMENT RESULTS ===\")\n",
    "print(f\"Total utterances: {len(vocalizations_df)}\")\n",
    "print(f\"Utterances assigned to categories: {(vocalizations_df['category'] != 'unknown').sum()}\")\n",
    "print(f\"Utterances without category: {(vocalizations_df['category'] == 'unknown').sum()}\")\n",
    "\n",
    "print(f\"\\n=== Total Words by Social Category ===\")\n",
    "print(words_by_category)\n",
    "print(f\"\\n=== Percentage of Words by Social Category ===\")\n",
    "print(words_by_category_pct)\n",
    "\n",
    "print(f\"\\n=== Words by Child and Category (first 10 children) ===\")\n",
    "print(words_by_child_cat.head(10))\n",
    "\n",
    "print(f\"\\n=== Percentage of Words by Child and Category (first 10 children) ===\")\n",
    "print(words_by_child_cat_pct.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the `assign_category` Function\n",
    "\n",
    "The `assign_category` function works as follows:\n",
    "\n",
    "#### **Input:**\n",
    "- **Vocalization**: `[start_time, end_time]` in seconds\n",
    "- **Segments**: Multiple segments with `[start_time_sec, end_time_sec]` for the same video\n",
    "\n",
    "#### **Overlap Detection Logic:**\n",
    "For an utterance and segment to overlap, **both** conditions must be true:\n",
    "1. `utterance_end > segment_start` (utterance continues past segment beginning)\n",
    "2. `utterance_start < segment_end` (utterance starts before segment ends)\n",
    "\n",
    "#### **Example:**\n",
    "```\n",
    "Utterance:  [10.5 -------- 15.2]\n",
    "Segment A:       [12.0 ----------- 20.0]  ✅ OVERLAPS\n",
    "Segment B: [5.0 ---- 9.0]                 ❌ No overlap\n",
    "Segment C:                   [18.0 -- 25.0] ❌ No overlap\n",
    "```\n",
    "\n",
    "#### **Overlap Duration Calculation:**\n",
    "```python\n",
    "overlap_start = max(utterance_start, segment_start)  # Latest start\n",
    "overlap_end = min(utterance_end, segment_end)        # Earliest end\n",
    "overlap_duration = overlap_end - overlap_start       # Duration of overlap\n",
    "```\n",
    "\n",
    "#### **Category Assignment:**\n",
    "- Find all overlapping segments\n",
    "- Calculate overlap duration for each\n",
    "- Assign the category of the segment with **maximum overlap**\n",
    "- If no overlaps found → category = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEBUG: Test assign_category function with examples ---\n",
    "print(\"=== DEBUGGING assign_category function ===\")\n",
    "\n",
    "# Take first few utterances as examples\n",
    "if len(vocalizations_df) > 0:\n",
    "    for i in range(min(3, len(vocalizations_df))):\n",
    "        row = vocalizations_df.iloc[i]\n",
    "        vid = row['video_id']\n",
    "        utterance_start = row['start_time']\n",
    "        utterance_end = row['end_time']\n",
    "        \n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(f\"Utterance: Video {vid}, [{utterance_start:.2f}, {utterance_end:.2f}]\")\n",
    "        print(f\"Duration: {utterance_end - utterance_start:.2f} seconds\")\n",
    "        \n",
    "        # Find overlapping segments\n",
    "        overlapping_segments = segments_lookup[\n",
    "            (segments_lookup['video_id'] == vid) &\n",
    "            (segments_lookup['end_time_sec'] > utterance_start) &\n",
    "            (segments_lookup['start_time_sec'] < utterance_end)\n",
    "        ]\n",
    "        \n",
    "        print(f\"Found {len(overlapping_segments)} overlapping segments:\")\n",
    "        \n",
    "        if len(overlapping_segments) > 0:\n",
    "            for idx, seg in overlapping_segments.iterrows():\n",
    "                overlap_start = max(utterance_start, seg['start_time_sec'])\n",
    "                overlap_end = min(utterance_end, seg['end_time_sec'])\n",
    "                overlap_duration = overlap_end - overlap_start\n",
    "                \n",
    "                print(f\"  Segment: [{seg['start_time_sec']:.2f}, {seg['end_time_sec']:.2f}] - {seg['category']}\")\n",
    "                print(f\"    Overlap: [{overlap_start:.2f}, {overlap_end:.2f}] = {overlap_duration:.2f} sec\")\n",
    "        else:\n",
    "            print(\"  No overlapping segments found!\")\n",
    "        \n",
    "        print(f\"Assigned category: {row.get('category', 'NOT_YET_ASSIGNED')}\")\n",
    "\n",
    "# Check for potential issues\n",
    "print(f\"\\n=== POTENTIAL ISSUES ===\")\n",
    "print(f\"Utterances with 'unknown' category: {(vocalizations_df['category'] == 'unknown').sum()}\")\n",
    "print(f\"Utterances with missing child_id: {vocalizations_df['child_id'].isna().sum()}\")\n",
    "\n",
    "# Check if vocalization column names match what we expect\n",
    "print(f\"\\nVocalization time columns: {[col for col in vocalizations_df.columns if 'time' in col.lower()]}\")\n",
    "print(f\"Segment time columns: {[col for col in segments_lookup.columns if 'time' in col.lower()]}\")\n",
    "\n",
    "# Show time ranges\n",
    "if len(vocalizations_df) > 0:\n",
    "    print(f\"\\nVocalization time range: {vocalizations_df['start_time'].min():.2f} - {vocalizations_df['end_time'].max():.2f}\")\n",
    "if len(segments_lookup) > 0:\n",
    "    print(f\"Segment time range: {segments_lookup['start_time_sec'].min():.2f} - {segments_lookup['end_time_sec'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        image_id  \\\n",
      "0  quantex_at_home_id255944_2022_03_08_01_000030   \n",
      "1  quantex_at_home_id255944_2022_03_08_01_000060   \n",
      "2  quantex_at_home_id255944_2022_03_08_01_000090   \n",
      "3  quantex_at_home_id255944_2022_03_08_01_000120   \n",
      "4  quantex_at_home_id255944_2022_03_08_01_000360   \n",
      "\n",
      "                            bbox_gt_1 bbox_gt_2 bbox_gt_3 person_age_gt_1  \\\n",
      "0   [1911.28, 500.33, 2304.0, 1079.9]       NaN       NaN           adult   \n",
      "1     [1297.97, 20.78, 1736.4, 575.0]       NaN       NaN           adult   \n",
      "2     [1927.6, 190.42, 2304.0, 780.5]       NaN       NaN           adult   \n",
      "3      [2045.0, 66.45, 2304.0, 565.2]       NaN       NaN           adult   \n",
      "4  [1097.67, 496.01, 1222.22, 645.69]       NaN       NaN           adult   \n",
      "\n",
      "  person_age_gt_2 person_age_gt_3  \n",
      "0             NaN             NaN  \n",
      "1             NaN             NaN  \n",
      "2             NaN             NaN  \n",
      "3             NaN             NaN  \n",
      "4             NaN             NaN  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the database\n",
    "db_path = '/home/nele_pauline_suffo/ProcessedData/quantex_annotations/annotations.db'\n",
    "interaction_conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Define the video IDs you want to filter\n",
    "video_ids = [5, 6, 7, 8, 11, 23]\n",
    "\n",
    "# Query annotations with selected columns\n",
    "query_annotations = f\"\"\"\n",
    "SELECT video_id, image_id, bbox, person_age\n",
    "FROM annotations\n",
    "WHERE video_id IN ({', '.join(map(str, video_ids))}) \n",
    "AND category_id = 10\n",
    "AND outside = 0\n",
    "\"\"\"\n",
    "\n",
    "# Query videos table for file names\n",
    "query_videos = f\"\"\"\n",
    "SELECT id, file_name\n",
    "FROM videos\n",
    "WHERE id IN ({', '.join(map(str, video_ids))})\n",
    "\"\"\"\n",
    "\n",
    "# Load data into DataFrames\n",
    "face_detections_gt = pd.read_sql(query_annotations, interaction_conn)\n",
    "videos_df = pd.read_sql(query_videos, interaction_conn)\n",
    "\n",
    "# Close the DB connection\n",
    "interaction_conn.close()\n",
    "\n",
    "# Merge the two DataFrames on video_id\n",
    "gt_df = face_detections_gt.merge(videos_df, left_on=\"video_id\", right_on=\"id\", how=\"left\")\n",
    "\n",
    "# Create combined image_id: file_name + \"_\" + zero-padded image_id\n",
    "gt_df[\"image_id\"] = gt_df.apply(lambda row: f\"{row['file_name'].replace('.mp4','')}_{int(row['image_id']):06d}\", axis=1)\n",
    "\n",
    "# map person_age infant to child\n",
    "gt_df[\"person_age\"] = gt_df[\"person_age\"].replace({\"infant\": \"child\"})\n",
    "\n",
    "# Drop the extra 'id' column if not needed\n",
    "gt_df.drop(columns=[\"id\", \"file_name\", \"video_id\"], inplace=True)\n",
    "gt_df.rename(columns={\"bbox\": \"bbox_gt\", \"person_age\": \"person_age_gt\"}, inplace=True)\n",
    "\n",
    "gt_df[\"gt_idx\"] = gt_df.groupby(\"image_id\").cumcount() + 1\n",
    "\n",
    "gt_df_wide = gt_df.pivot(index=\"image_id\", columns=\"gt_idx\")\n",
    "gt_df_wide.columns = [f\"{col[0]}_{col[1]}\" for col in gt_df_wide.columns]\n",
    "gt_df_wide.reset_index(inplace=True)\n",
    "\n",
    "print(gt_df_wide.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        image_id  \\\n",
      "0  quantex_at_home_id255944_2022_03_08_01_000030   \n",
      "1  quantex_at_home_id255944_2022_03_08_01_000060   \n",
      "2  quantex_at_home_id255944_2022_03_08_01_000090   \n",
      "3  quantex_at_home_id255944_2022_03_08_01_000120   \n",
      "4  quantex_at_home_id255944_2022_03_08_01_000360   \n",
      "\n",
      "                             bbox_pred_1 bbox_pred_2 bbox_pred_3 bbox_pred_4  \\\n",
      "0   [1928.706, 534.306, 375.294, 547.87]         NaN         NaN         NaN   \n",
      "1   [1292.824, 33.548, 434.423, 543.073]         NaN         NaN         NaN   \n",
      "2  [1929.726, 192.167, 372.645, 562.045]         NaN         NaN         NaN   \n",
      "3   [2050.465, 64.385, 253.136, 498.399]         NaN         NaN         NaN   \n",
      "4   [1113.687, 496.063, 109.06, 156.085]         NaN         NaN         NaN   \n",
      "\n",
      "  person_age_pred_1 person_age_pred_2 person_age_pred_3 person_age_pred_4  \n",
      "0             adult               NaN               NaN               NaN  \n",
      "1             adult               NaN               NaN               NaN  \n",
      "2             adult               NaN               NaN               NaN  \n",
      "3             adult               NaN               NaN               NaN  \n",
      "4             adult               NaN               NaN               NaN  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load JSON\n",
    "with open(\"/home/nele_pauline_suffo/outputs/face_detections/yolo12l_validation_20250822_112418/predictions.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_pred = pd.DataFrame(data)\n",
    "\n",
    "# Extract video_id and frame_number from image_id\n",
    "def parse_video_id(image_id):\n",
    "    # example: quantex_at_home_id255944_2022_03_08_01_000000 return 000000 without trailing zeros\n",
    "    parts = image_id.split(\"_\")\n",
    "    # video_id from idXXXXX\n",
    "    vid = int(parts[3].replace(\"id\", \"\"))  # \"id255944\" → 255944\n",
    "    return vid\n",
    "\n",
    "def parse_frame_number(image_id):\n",
    "    # last part is frame number\n",
    "    return int(image_id.split(\"_\")[-1])\n",
    "\n",
    "df_pred[\"frame_number\"] = df_pred[\"image_id\"].apply(parse_frame_number)\n",
    "# filter to only keep predictions with score greater than 0.25\n",
    "df_pred = df_pred[df_pred[\"score\"] >= 0.25]\n",
    "\n",
    "# Rename columns\n",
    "df_pred.rename(columns={\"score\": \"confidence_score\", \"category_id\": \"person_age_pred\", \"bbox\": \"bbox_pred\"}, inplace=True)\n",
    "\n",
    "# Add extra columns\n",
    "df_pred[\"person_age_pred\"] = df_pred[\"person_age_pred\"].replace({1: \"child\", 2: \"adult\"})\n",
    "# Reorder columns\n",
    "final_columns = [\"image_id\", \"bbox_pred\", \"person_age_pred\"]\n",
    "df_pred = df_pred[final_columns]\n",
    "\n",
    "df_pred[\"gt_idx\"] = df_pred.groupby(\"image_id\").cumcount() + 1\n",
    "\n",
    "df_pred_wide = df_pred.pivot(index=\"image_id\", columns=\"gt_idx\")\n",
    "df_pred_wide.columns = [f\"{col[0]}_{col[1]}\" for col in df_pred_wide.columns]\n",
    "df_pred_wide.reset_index(inplace=True)\n",
    "\n",
    "print(df_pred_wide.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join both tables on image_id (outer join)\n",
    "df_final = df_pred_wide.merge(gt_df_wide, on=\"image_id\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>bbox_pred_1</th>\n",
       "      <th>bbox_pred_2</th>\n",
       "      <th>bbox_pred_3</th>\n",
       "      <th>bbox_pred_4</th>\n",
       "      <th>person_age_pred_1</th>\n",
       "      <th>person_age_pred_2</th>\n",
       "      <th>person_age_pred_3</th>\n",
       "      <th>person_age_pred_4</th>\n",
       "      <th>bbox_gt_1</th>\n",
       "      <th>bbox_gt_2</th>\n",
       "      <th>bbox_gt_3</th>\n",
       "      <th>person_age_gt_1</th>\n",
       "      <th>person_age_gt_2</th>\n",
       "      <th>person_age_gt_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01_002010</td>\n",
       "      <td>[2100.492, 1188.071, 86.245, 105.115]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01_003690</td>\n",
       "      <td>[1991.526, 0.33, 125.2, 123.789]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01_011220</td>\n",
       "      <td>[127.639, 286.176, 71.385, 75.641]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01_011460</td>\n",
       "      <td>[115.714, 0.0, 1091.078, 425.623]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01_011850</td>\n",
       "      <td>[2002.212, 0.0, 301.549, 530.796]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_008310</td>\n",
       "      <td>[2251.084, 1105.558, 52.916, 172.597]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_009270</td>\n",
       "      <td>[2248.686, 101.383, 55.314, 189.439]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_009630</td>\n",
       "      <td>[2086.594, 0.0, 217.204, 161.988]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_010290</td>\n",
       "      <td>[396.361, 0.0, 745.816, 467.368]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_011610</td>\n",
       "      <td>[2148.985, 515.846, 53.298, 50.473]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_id  \\\n",
       "13   quantex_at_home_id255944_2022_03_08_01_002010   \n",
       "41   quantex_at_home_id255944_2022_03_08_01_003690   \n",
       "46   quantex_at_home_id255944_2022_03_08_01_011220   \n",
       "47   quantex_at_home_id255944_2022_03_08_01_011460   \n",
       "54   quantex_at_home_id255944_2022_03_08_01_011850   \n",
       "..                                             ...   \n",
       "668  quantex_at_home_id260275_2022_04_28_01_008310   \n",
       "673  quantex_at_home_id260275_2022_04_28_01_009270   \n",
       "675  quantex_at_home_id260275_2022_04_28_01_009630   \n",
       "676  quantex_at_home_id260275_2022_04_28_01_010290   \n",
       "677  quantex_at_home_id260275_2022_04_28_01_011610   \n",
       "\n",
       "                               bbox_pred_1 bbox_pred_2 bbox_pred_3  \\\n",
       "13   [2100.492, 1188.071, 86.245, 105.115]         NaN         NaN   \n",
       "41        [1991.526, 0.33, 125.2, 123.789]         NaN         NaN   \n",
       "46      [127.639, 286.176, 71.385, 75.641]         NaN         NaN   \n",
       "47       [115.714, 0.0, 1091.078, 425.623]         NaN         NaN   \n",
       "54       [2002.212, 0.0, 301.549, 530.796]         NaN         NaN   \n",
       "..                                     ...         ...         ...   \n",
       "668  [2251.084, 1105.558, 52.916, 172.597]         NaN         NaN   \n",
       "673   [2248.686, 101.383, 55.314, 189.439]         NaN         NaN   \n",
       "675      [2086.594, 0.0, 217.204, 161.988]         NaN         NaN   \n",
       "676       [396.361, 0.0, 745.816, 467.368]         NaN         NaN   \n",
       "677    [2148.985, 515.846, 53.298, 50.473]         NaN         NaN   \n",
       "\n",
       "    bbox_pred_4 person_age_pred_1 person_age_pred_2 person_age_pred_3  \\\n",
       "13          NaN             child               NaN               NaN   \n",
       "41          NaN             adult               NaN               NaN   \n",
       "46          NaN             child               NaN               NaN   \n",
       "47          NaN             child               NaN               NaN   \n",
       "54          NaN             adult               NaN               NaN   \n",
       "..          ...               ...               ...               ...   \n",
       "668         NaN             adult               NaN               NaN   \n",
       "673         NaN             adult               NaN               NaN   \n",
       "675         NaN             child               NaN               NaN   \n",
       "676         NaN             adult               NaN               NaN   \n",
       "677         NaN             adult               NaN               NaN   \n",
       "\n",
       "    person_age_pred_4 bbox_gt_1 bbox_gt_2 bbox_gt_3 person_age_gt_1  \\\n",
       "13                NaN       NaN       NaN       NaN             NaN   \n",
       "41                NaN       NaN       NaN       NaN             NaN   \n",
       "46                NaN       NaN       NaN       NaN             NaN   \n",
       "47                NaN       NaN       NaN       NaN             NaN   \n",
       "54                NaN       NaN       NaN       NaN             NaN   \n",
       "..                ...       ...       ...       ...             ...   \n",
       "668               NaN       NaN       NaN       NaN             NaN   \n",
       "673               NaN       NaN       NaN       NaN             NaN   \n",
       "675               NaN       NaN       NaN       NaN             NaN   \n",
       "676               NaN       NaN       NaN       NaN             NaN   \n",
       "677               NaN       NaN       NaN       NaN             NaN   \n",
       "\n",
       "    person_age_gt_2 person_age_gt_3  \n",
       "13              NaN             NaN  \n",
       "41              NaN             NaN  \n",
       "46              NaN             NaN  \n",
       "47              NaN             NaN  \n",
       "54              NaN             NaN  \n",
       "..              ...             ...  \n",
       "668             NaN             NaN  \n",
       "673             NaN             NaN  \n",
       "675             NaN             NaN  \n",
       "676             NaN             NaN  \n",
       "677             NaN             NaN  \n",
       "\n",
       "[101 rows x 15 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final[\"bbox_gt_1\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "missclassified_df = df_final[\n",
    "    (df_final[\"person_age_pred_1\"] != df_final[\"person_age_gt_1\"]) &\n",
    "    df_final[\"person_age_pred_1\"].notna() &\n",
    "    df_final[\"person_age_gt_1\"].notna() &\n",
    "    df_final[\"person_age_gt_2\"].isna()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>person_age_pred_1</th>\n",
       "      <th>person_age_gt_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01_003630</td>\n",
       "      <td>adult</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>quantex_at_home_id255944_2022_03_08_01_014160</td>\n",
       "      <td>adult</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>quantex_at_home_id255944_2022_03_10_01_009660</td>\n",
       "      <td>adult</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_001140</td>\n",
       "      <td>child</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_001290</td>\n",
       "      <td>child</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_001560</td>\n",
       "      <td>child</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_001830</td>\n",
       "      <td>child</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_002280</td>\n",
       "      <td>child</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_002340</td>\n",
       "      <td>child</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_002550</td>\n",
       "      <td>child</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_002790</td>\n",
       "      <td>child</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_003000</td>\n",
       "      <td>child</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_009210</td>\n",
       "      <td>child</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_009570</td>\n",
       "      <td>adult</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_012360</td>\n",
       "      <td>adult</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_28_01_012390</td>\n",
       "      <td>adult</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_id person_age_pred_1  \\\n",
       "39   quantex_at_home_id255944_2022_03_08_01_003630             adult   \n",
       "65   quantex_at_home_id255944_2022_03_08_01_014160             adult   \n",
       "109  quantex_at_home_id255944_2022_03_10_01_009660             adult   \n",
       "622  quantex_at_home_id260275_2022_04_28_01_001140             child   \n",
       "625  quantex_at_home_id260275_2022_04_28_01_001290             child   \n",
       "629  quantex_at_home_id260275_2022_04_28_01_001560             child   \n",
       "634  quantex_at_home_id260275_2022_04_28_01_001830             child   \n",
       "639  quantex_at_home_id260275_2022_04_28_01_002280             child   \n",
       "640  quantex_at_home_id260275_2022_04_28_01_002340             child   \n",
       "644  quantex_at_home_id260275_2022_04_28_01_002550             child   \n",
       "645  quantex_at_home_id260275_2022_04_28_01_002790             child   \n",
       "650  quantex_at_home_id260275_2022_04_28_01_003000             child   \n",
       "671  quantex_at_home_id260275_2022_04_28_01_009210             child   \n",
       "674  quantex_at_home_id260275_2022_04_28_01_009570             adult   \n",
       "681  quantex_at_home_id260275_2022_04_28_01_012360             adult   \n",
       "682  quantex_at_home_id260275_2022_04_28_01_012390             adult   \n",
       "\n",
       "    person_age_gt_1  \n",
       "39            child  \n",
       "65            child  \n",
       "109           child  \n",
       "622           adult  \n",
       "625           adult  \n",
       "629           adult  \n",
       "634           adult  \n",
       "639           adult  \n",
       "640           adult  \n",
       "644           adult  \n",
       "645           adult  \n",
       "650           adult  \n",
       "671           adult  \n",
       "674           child  \n",
       "681           child  \n",
       "682           child  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missclassified_df[[\"image_id\", \"person_age_pred_1\", \"persovn_age_gt_1\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
